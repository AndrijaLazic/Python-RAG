{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21af21b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from unstructured.partition.pdf import partition_pdf\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "file_path = '../content/output/' + 'fmea_handbook.pdf'\n",
    "output_path = '../content/output/'\n",
    "\n",
    "\n",
    "output_dir = Path(output_path)\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "output_file_path = output_dir / \"chunks.pkl\"\n",
    "\n",
    "OLLAMA_URL = \"http://127.0.0.1:11434\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed58e8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No languages specified, defaulting to English.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `max_size` parameter is deprecated and will be removed in v4.26. Please specify in `size['longest_edge'] instead`.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Reference: https://docs.unstructured.io/open-source/core-functionality/chunking\n",
    "chunks = partition_pdf(\n",
    "    filename=file_path,\n",
    "    infer_table_structure=True,            # extract tables\n",
    "    strategy=\"hi_res\",                     # mandatory to infer tables\n",
    "\n",
    "    extract_image_block_types=[\"Image\"],   # Add 'Table' to list to extract image of tables\n",
    "    # image_output_dir_path=output_path,   # if None, images and tables will saved in base64\n",
    "\n",
    "    extract_image_block_to_payload=True,   # if true, will extract base64 for API usage\n",
    "\n",
    "    chunking_strategy=\"by_title\",          # or 'basic'\n",
    "    max_characters=10000,                  # defaults to 500\n",
    "    combine_text_under_n_chars=2000,       # defaults to 0\n",
    "    new_after_n_chars=6000,\n",
    "\n",
    "    # extract_images_in_pdf=True,          # deprecated\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69f56266",
   "metadata": {},
   "outputs": [],
   "source": [
    "if chunks is not None and len(chunks) > 0:\n",
    "    with open(output_file_path, \"wb\") as f:\n",
    "        pickle.dump(chunks, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67c47153",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_file_path, \"rb\") as f:\n",
    "    chunks = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6bf2b91b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"<class 'unstructured.documents.elements.CompositeElement'>\"}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set([str(type(el)) for el in chunks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25cb93ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate tables from texts\n",
    "tables = []\n",
    "texts = []\n",
    "\n",
    "for chunk in chunks:\n",
    "    if \"Table\" in str(type(chunk)):\n",
    "        tables.append(chunk)\n",
    "\n",
    "    if \"CompositeElement\" in str(type((chunk))):\n",
    "        texts.append(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8628730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the images from the CompositeElement objects\n",
    "def get_images_base64(chunks):\n",
    "    images_b64 = []\n",
    "    for chunk in chunks:\n",
    "        if \"CompositeElement\" in str(type(chunk)):\n",
    "            chunk_els = chunk.metadata.orig_elements\n",
    "            for el in chunk_els:\n",
    "                if \"Image\" in str(type(el)):\n",
    "                    images_b64.append(el.metadata.image_base64)\n",
    "    return images_b64\n",
    "\n",
    "images = get_images_base64(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23ff72a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "from PIL import Image\n",
    "import io\n",
    "import os\n",
    "\n",
    "images_dir = output_dir / \"images\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for i, img_b64 in enumerate(images):\n",
    "    image_data = base64.b64decode(img_b64)\n",
    "    image = Image.open(io.BytesIO(image_data)).convert(\"RGB\")  # Convert to RGB for JPEG\n",
    "\n",
    "    file_path = os.path.join(images_dir, f\"image_{i+1}.jpg\")\n",
    "    image.save(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5ebba9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove images you dont need by hand and load them again\n",
    "images = []\n",
    "for filename in sorted(os.listdir(images_dir)):\n",
    "    file_path = os.path.join(images_dir, filename)\n",
    "    with open(file_path, \"rb\") as image_file:\n",
    "        encoded = base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "        images.append(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6875cd4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collections=[CollectionDescription(name='my_first_rag_collection')]\n"
     ]
    }
   ],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import Distance, VectorParams\n",
    "import requests\n",
    "\n",
    "# If you're using local Qdrant on default port\n",
    "client = QdrantClient(host=\"qdrant\", port=6333)\n",
    "\n",
    "def embed_texts_ollama(texts, model=\"mxbai-embed-large:335m\"):\n",
    "    url = \"http://host.docker.internal:11434/api/embeddings\"\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"prompt\": texts if isinstance(texts, str) else texts[0]\n",
    "    }\n",
    "\n",
    "    if isinstance(texts, list):\n",
    "        embeddings = []\n",
    "        for text in texts:\n",
    "            payload[\"prompt\"] = text\n",
    "            response = requests.post(url, json=payload)\n",
    "            response.raise_for_status()\n",
    "            embeddings.append(response.json()[\"embedding\"])\n",
    "        return embeddings\n",
    "    else:\n",
    "        response = requests.post(url, json=payload)\n",
    "        response.raise_for_status()\n",
    "        return [response.json()[\"embedding\"]]\n",
    "\n",
    "collection_name = \"my_first_rag_collection\"\n",
    "\n",
    "if not client.collection_exists(collection_name=collection_name):\n",
    "    client.create_collection(\n",
    "        collection_name=collection_name,\n",
    "        vectors_config=VectorParams(size=1024, distance=Distance.COSINE)  # 1024 is the dimension for mxbai\n",
    "    )\n",
    "\n",
    "\n",
    "print(client.get_collections())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c12cbf00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client.models import PointStruct\n",
    "import uuid\n",
    "\n",
    "def upload_chunks_to_qdrant(chunks, collection_name):\n",
    "    texts = [chunk.page_content for chunk in chunks]\n",
    "    embeddings = embed_texts_ollama(texts)  # Step 1\n",
    "    \n",
    "    points = []\n",
    "    for i, (chunk, embedding) in enumerate(zip(chunks, embeddings)):\n",
    "        payload = {\n",
    "            \"text\": chunk.page_content,\n",
    "            **chunk.metadata  # Optional: add page_number, category, etc.\n",
    "        }\n",
    "\n",
    "        points.append(PointStruct(\n",
    "            id=str(uuid.uuid4()),\n",
    "            vector=embedding,\n",
    "            payload=payload\n",
    "        ))\n",
    "\n",
    "    client.upsert(collection_name=collection_name, points=points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2275f92",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax. Perhaps you forgot a comma? (3599291810.py, line 5)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mquery=query_vector\u001b[39m\n          ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax. Perhaps you forgot a comma?\n"
     ]
    }
   ],
   "source": [
    "query_vector = embed_texts_ollama(\"FMEA\")[0]\n",
    "\n",
    "results = client.query_points(\n",
    "    collection_name=collection_name,\n",
    "    query=query_vector,\n",
    "    limit=5\n",
    ")\n",
    "\n",
    "for result in results:\n",
    "    print(result.payload[\"text\"], result.score)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
